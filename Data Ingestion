How to connect to on-prem sources from databricks ?
You can use JDBC or ODBC connectors to connect to on-prem soruces

What are they ?
The JDBC connector in Databricks is a special software driver that enables applications, tools, and clients to connect with Databricks using the well-known Java Database Connectivity (JDBC) protocol. 
JDBC is an industry-standard API that allows Java-based programs to access and interact with databases and big data platforms.
in short the JDBC connector acts as a bridge between Databricks and any application or tool that supports JDBC, enabling smooth data access, SQL querying, and integration with Java-based frameworks and BI tools

how does it work ?
You configure your connection by providing details like the Databricks workspace/server hostname, HTTP path, authentication token, and any other required parameters.
Tools or code using JDBC send queries and commands through this connector, which then communicates securely with the Databricks cluster or SQL Warehouse to process data.

Which one to choose ?
Use JDBC:
If you are working within a Java-based environment (such as Databricks notebooks, which run on JVM).
If you need cross-platform compatibility.
If you want easier integration and less OS-specific configuration.
JDBC drivers are typically the standard and most supported option for Databricks, and most database vendors provide JDBC drivers for integration.

Use ODBC:
If you need to connect through a tool, application, or client that does not support Java or requires native, platform-specific integration.
If you’re working with a legacy system or BI tool that only supports ODBC.
ODBC is less common for direct Databricks notebook usage but 


====================================================================================================================================

How to read data from a table in on prem sql server using databricks notebook?

1. Ensure Network Connectivity
Your Databricks cluster must have network access to your on-prem SQL Server.
This usually requires setting up a VPN, ExpressRoute, or similar connection between your on-prem environment and the Databricks environment (on Azure/AWS) so your cluster can reach the SQL Server IP/hostname and port (typically 1433).

2. Get the SQL Server JDBC Driver
The JDBC driver for SQL Server (com.microsoft.sqlserver.jdbc.SQLServerDriver) must be available on your cluster.
On Databricks, you can usually add this dependency via Maven, or by uploading the JAR to your cluster under Libraries.

3. Sample code
# Set parameters
jdbc_hostname = "YOUR_SQLSERVER_HOST_OR_IP"
jdbc_port = 1433
database = "YOUR_DB_NAME"
username = "YOUR_USERNAME"
password = "YOUR_PASSWORD"
table = "employees"  # Or use "schema.employees" if needed

# JDBC URL
jdbc_url = f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={database}"

# JDBC driver class for SQL Server
driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"

# Read into DataFrame
employees_df = (
    spark.read
    .format("jdbc")
    .option("url", jdbc_url)
    .option("dbtable", table)
    .option("user", username)
    .option("password", password)
    .option("driver", driver)
    .load()
)

# Show the data
employees_df.show()

SelfLearn : How to use secrets to avoid hard quoting of username and passwords.
========================================================================================================================================================================

Using ABFC paths to connect to external data sources. In this case lets take onelake in microsoft fabric as example :

1. Set Up Your Service Principal in Azure
Go to Azure Portal → Microsoft Entra ID → App registrations.
Create a new app registration (service principal).
Note the Application (client) ID, Directory (tenant) ID, and create a client secret for this service principal.

2. Enable Service Principal Access in Fabric
In Microsoft Fabric Admin Portal, go to Tenant settings.
For both Power BI APIs (under Developer settings) and OneLake settings, enable service principals for your organization.
Go to your Fabric workspace containing the Lakehouse → Manage access.
Add the service principal to your workspace and assign at least the Contributor role.

3. Configure Required Info in Databricks
You will need:
tenant_id (Directory ID)
client_id (Application ID)
client_secret (Service Principal secret)
The ABFS path to your Lakehouse (abfss://... as shown earlier)
The storage account portion is typically onelake.dfs.fabric.microsoft.com for Fabric.

4. Sample code

# Service Principal Credentials
tenant_id = "<Your-Tenant-ID>"
client_id = "<Your-Client-ID>"
client_secret = "<Your-Client-Secret>"
storage_account = "onelake.dfs.fabric.microsoft.com"

# Set Spark configs for OAuth using Service Principal
spark.conf.set(f"fs.azure.account.auth.type.{storage_account}", "OAuth")
spark.conf.set(f"fs.azure.account.oauth.provider.type.{storage_account}", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set(f"fs.azure.account.oauth2.client.id.{storage_account}", client_id)
spark.conf.set(f"fs.azure.account.oauth2.client.secret.{storage_account}", client_secret)
spark.conf.set(f"fs.azure.account.oauth2.client.endpoint.{storage_account}", f"https://login.microsoftonline.com/{tenant_id}/oauth2/token")

lakehouse_path = "abfss://<Workspace>@onelake.dfs.fabric.microsoft.com/<Lakehouse>.lakehouse/Files/employees.csv"
# Read data
df = spark.read.format("csv").option("header", "true").load(lakehouse_path)
df.show()
# Write data
df.write.format("parquet").mode("overwrite").save("abfss://<Workspace>@onelake.dfs.fabric.microsoft.com/<Lakehouse>.lakehouse/Files/backup/")

SelfLearn : It is always recomended to use service principle to connect to external services in production environment. Try using service principle.
For ref : https://microsoft.github.io/TechExcel-Fabric-with-Databricks-for-Data-Analytics/

=================================================================================================================================================================

Load files recursively from two different workspaces into a lakehouse using wildcards

Imagine a scenario in which you are collaborating with two colleagues who have stored sales data in two separate lakehouses, each within a different workspace and under distinct names but same schema. 
The data is saved in folders containing numerous parquet files. 
One colleague saved the parquet files with names starting with "data-", while the other used Spark, which typically saves files with names beginning with "part-". 
Your task is to read this data in a notebook for further analysis.

//Sample Code
paths = [
    "abfss://<workspace1_id>@onelake.dfs.fabric.microsoft.com/<lakehouse2_id>/Files/<folder_name>/",
    "abfss://<workspace2_id>@onelake.dfs.fabric.microsoft.com/<lakehouse2_id>/Files/<folder_name1>/<folder_name2>"
]

df = (spark.read
      .option("pathGlobFilter", "{data*.parquet,part*.parquet}") # only files starting with data or part
      .option("recursiveFileLookup", "true") #recursively load from nested sub folders
      .parquet(*paths))

//PathGlobeFilter

pathGlobFilter is used to only include files with file names matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.
To load files with paths matching a given glob pattern while keeping the behavior of partition discovery, you can use:
df = spark.read.load("examples/src/main/resources/dir1",
                     format="parquet", pathGlobFilter="*.parquet")

//Recursive File Lookup
recursiveFileLookup is used to recursively load files and it disables partition inferring. Its default value is false. 
If data source explicitly specifies the partitionSpec when recursiveFileLookup is true, exception will be thrown.
To load all files recursively, you can use:
recursive_loaded_df = spark.read.format("parquet")\
    .option("recursiveFileLookup", "true")\
    .load("examples/src/main/resources/dir1")

//Ignore Missing Files
Spark allows you to use the configuration spark.sql.files.ignoreMissingFiles or the data source option ignoreMissingFiles to ignore missing files while reading data from files. 
Here, missing file really means the deleted file under directory after you construct the DataFrame. 
When set to true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned.

// Modification Time Path Filters
modifiedBefore and modifiedAfter are options that can be applied together or separately in order to achieve greater granularity over which files may load during a Spark batch query. 
(Note that Structured Streaming file sources don’t support these options.)

## modifiedBefore: an optional timestamp to only include files with modification times occurring before the specified time. 
The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)
## modifiedAfter: an optional timestamp to only include files with modification times occurring after the specified time. 
The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)
When a timezone option is not provided, the timestamps will be interpreted according to the Spark session timezone (spark.sql.session.timeZone).

To load files with paths matching a given modified time range, you can use:
df = spark.read.load("examples/src/main/resources/dir1",
                     format="parquet", modifiedBefore="2050-07-01T08:30:00")

# Only load files modified after 06/01/2050 @ 08:30:00
df = spark.read.load("examples/src/main/resources/dir1",
                     format="parquet", modifiedAfter="2050-06-01T08:30:00")
===============================================================================================================================================================================









