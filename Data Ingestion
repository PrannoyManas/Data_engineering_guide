How to connect to on-prem sources from databricks ?
You can use JDBC or ODBC connectors to connect to on-prem soruces

What are they ?
The JDBC connector in Databricks is a special software driver that enables applications, tools, and clients to connect with Databricks using the well-known Java Database Connectivity (JDBC) protocol. 
JDBC is an industry-standard API that allows Java-based programs to access and interact with databases and big data platforms.
in short the JDBC connector acts as a bridge between Databricks and any application or tool that supports JDBC, enabling smooth data access, SQL querying, and integration with Java-based frameworks and BI tools

how does it work ?
You configure your connection by providing details like the Databricks workspace/server hostname, HTTP path, authentication token, and any other required parameters.
Tools or code using JDBC send queries and commands through this connector, which then communicates securely with the Databricks cluster or SQL Warehouse to process data.

Which one to choose ?
Use JDBC:
If you are working within a Java-based environment (such as Databricks notebooks, which run on JVM).
If you need cross-platform compatibility.
If you want easier integration and less OS-specific configuration.
JDBC drivers are typically the standard and most supported option for Databricks, and most database vendors provide JDBC drivers for integration.

Use ODBC:
If you need to connect through a tool, application, or client that does not support Java or requires native, platform-specific integration.
If you’re working with a legacy system or BI tool that only supports ODBC.
ODBC is less common for direct Databricks notebook usage but 


====================================================================================================================================

How to read data from a table in on prem sql server using databricks notebook?

1. Ensure Network Connectivity
Your Databricks cluster must have network access to your on-prem SQL Server.
This usually requires setting up a VPN, ExpressRoute, or similar connection between your on-prem environment and the Databricks environment (on Azure/AWS) so your cluster can reach the SQL Server IP/hostname and port (typically 1433).

2. Get the SQL Server JDBC Driver
The JDBC driver for SQL Server (com.microsoft.sqlserver.jdbc.SQLServerDriver) must be available on your cluster.
On Databricks, you can usually add this dependency via Maven, or by uploading the JAR to your cluster under Libraries.

3. Sample code
# Set parameters
jdbc_hostname = "YOUR_SQLSERVER_HOST_OR_IP"
jdbc_port = 1433
database = "YOUR_DB_NAME"
username = "YOUR_USERNAME"
password = "YOUR_PASSWORD"
table = "employees"  # Or use "schema.employees" if needed

# JDBC URL
jdbc_url = f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={database}"

# JDBC driver class for SQL Server
driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"

# Read into DataFrame
employees_df = (
    spark.read
    .format("jdbc")
    .option("url", jdbc_url)
    .option("dbtable", table)
    .option("user", username)
    .option("password", password)
    .option("driver", driver)
    .load()
)

# Show the data
employees_df.show()

SelfLearn : How to use secrets to avoid hard quoting of username and passwords.
========================================================================================================================================================================

Using ABFC paths to connect to external data sources. In this case lets take onelake in microsoft fabric as example :

1. Set Up Your Service Principal in Azure
Go to Azure Portal → Microsoft Entra ID → App registrations.
Create a new app registration (service principal).
Note the Application (client) ID, Directory (tenant) ID, and create a client secret for this service principal.

2. Enable Service Principal Access in Fabric
In Microsoft Fabric Admin Portal, go to Tenant settings.
For both Power BI APIs (under Developer settings) and OneLake settings, enable service principals for your organization.
Go to your Fabric workspace containing the Lakehouse → Manage access.
Add the service principal to your workspace and assign at least the Contributor role.

3. Configure Required Info in Databricks
You will need:
tenant_id (Directory ID)
client_id (Application ID)
client_secret (Service Principal secret)
The ABFS path to your Lakehouse (abfss://... as shown earlier)
The storage account portion is typically onelake.dfs.fabric.microsoft.com for Fabric.

4. Sample code

# Service Principal Credentials
tenant_id = "<Your-Tenant-ID>"
client_id = "<Your-Client-ID>"
client_secret = "<Your-Client-Secret>"
storage_account = "onelake.dfs.fabric.microsoft.com"

# Set Spark configs for OAuth using Service Principal
spark.conf.set(f"fs.azure.account.auth.type.{storage_account}", "OAuth")
spark.conf.set(f"fs.azure.account.oauth.provider.type.{storage_account}", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set(f"fs.azure.account.oauth2.client.id.{storage_account}", client_id)
spark.conf.set(f"fs.azure.account.oauth2.client.secret.{storage_account}", client_secret)
spark.conf.set(f"fs.azure.account.oauth2.client.endpoint.{storage_account}", f"https://login.microsoftonline.com/{tenant_id}/oauth2/token")

lakehouse_path = "abfss://<Workspace>@onelake.dfs.fabric.microsoft.com/<Lakehouse>.lakehouse/Files/employees.csv"
# Read data
df = spark.read.format("csv").option("header", "true").load(lakehouse_path)
df.show()
# Write data
df.write.format("parquet").mode("overwrite").save("abfss://<Workspace>@onelake.dfs.fabric.microsoft.com/<Lakehouse>.lakehouse/Files/backup/")

SelfLearn : It is always recomended to use service principle to connect to external services in production environment. Try using service principle.
For ref : https://microsoft.github.io/TechExcel-Fabric-with-Databricks-for-Data-Analytics/

=================================================================================================================================================================







